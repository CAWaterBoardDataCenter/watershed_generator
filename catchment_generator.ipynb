{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P align=\"center\"><img src=\"https://www.fs.usda.gov/Internet/FSE_MEDIA/fseprd897983.png\" width = 800 height =200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body>\n",
    "<h1 style=\"background-color: #022851;\"><center>\n",
    "    <br><font size=\"+3.5\">\n",
    "    <font color=#F8F8FF><b>Generate Catchments and PRMS Modeled Flows by Locations of Interest (LOIs) </b></font>\n",
    "   </font>\n",
    "    <br><font size=\"+1\">\n",
    "    <font color=#F8F8FF></b> State Water Resources Control Board </font>\n",
    "   </font> \n",
    "    <br><font size=\"+1\">\n",
    "    <font color=#F8F8FF></b> Cannabis Instream Flows Unit </font> <br>\n",
    "       </font> \n",
    "    <br><font size=\"+1\">\n",
    "    <font color=#F8F8FF><b>Authors:</b> Jaspreet S. Gill, P.E. , Ryan Hankins</font>\n",
    "   </font> \n",
    "    </center>\n",
    "</h1>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook takes a user specified Digital Elevation Model (DEM) and pour points based on any Locations of Interest (LOI) to generate respective drainage catchment polygons. The notebook also takes the user specified Precipitation Runoff Modeling System (PRMS) model grid shapefile, HRU streamflow out csv, and the generated catchment areas of the specified pour points to output PRMS unimpaired flow estimates by each LOI.  All inputs can be generated with the retrieve_files.ipynb script in this repository. See the Inputs section of this notebook for more details.  Please contact Ryan Hankins at ryan.hankins@waterboards.ca.gov for any questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Imports and extensions</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import python libraries\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import sys, os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Inputs</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE NAME in the quotes to match the name of the geodatabase that has the pour points shapefile, dem raster and model grid\n",
    "#input_workspace = './LOI_FILES.gpkg'\n",
    "input_workspace = os.getcwd()\n",
    "\n",
    "# Change the NAME to match the large unimpaired flows by HRU file\n",
    "hru_streamflow_csv = \"SELECT * FROM EEL_HRU_STREAMFLOW_OUT_2002_2022\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn input files into geopandas dataframe/ rasterio object\n",
    "\n",
    "pour_points = gpd.read_file(r\"example.shp\") #change this to your pour points shapefile\n",
    "model_grid = gpd.read_file(r\"example.zip\") #change this to your model grid shapefile\n",
    "dem_raster = r\"example.tif\" # works with a .tif or .tiff file. Make sure the dem is not too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the attribute column in the pour points shapefile that holds the pour point IDs\n",
    "\n",
    "pour_point_id = \"StationCod\" # Change this to your pour point ID column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Projected coordinate system - NAD 1983 UTM Zone 10N</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the projection to NAD83 10N\n",
    "pour_points_utmz10N = pour_points.to_crs(crs=26910)\n",
    "model_grid_utmz10N = model_grid.to_crs(crs=26910)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Clip pour points to watershed model grid</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clip the pour_points the model grid\n",
    "pour_points_clipped = gpd.clip(pour_points_utmz10N, model_grid_utmz10N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pour_points_clipped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Generate Pour Point Catchment Areas</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read elevation raster\n",
    "\n",
    "from pysheds.grid import Grid\n",
    "\n",
    "grid = Grid.from_raster(dem_raster, crs = 26910) #make sure crs is in NAD83 zone 10\n",
    "dem = grid.read_raster(dem_raster, crs =26910)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition the DEM (this may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill pits in DEM\n",
    "pit_filled_dem = grid.fill_pits(dem)\n",
    "\n",
    "# Fill depressions in DEM\n",
    "flooded_dem = grid.fill_depressions(pit_filled_dem)\n",
    "    \n",
    "# Resolve flats in DEM\n",
    "inflated_dem = grid.resolve_flats(flooded_dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Flow Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use arcgis standard direction mapping\n",
    "\n",
    "dirmap = (64, 128, 1, 2, 4, 8, 16, 32)\n",
    "    \n",
    "# Compute flow directions\n",
    "\n",
    "fdir = grid.flowdir(inflated_dem, dirmap=dirmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL - plot flow direction map\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "fig.patch.set_alpha(0)\n",
    "\n",
    "plt.imshow(fdir, extent=grid.extent, cmap='viridis', zorder=2)\n",
    "boundaries = ([0] + sorted(list(dirmap)))\n",
    "plt.colorbar(boundaries= boundaries,\n",
    "             values=sorted(dirmap))\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Flow direction grid', size=14)\n",
    "plt.grid(zorder=-1)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate flow accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate flow accumulation\n",
    "\n",
    "acc = grid.accumulation(fdir, dirmap=dirmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL - plot flow accumulation map\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig.patch.set_alpha(0)\n",
    "plt.grid('on', zorder=0)\n",
    "im = ax.imshow(acc, extent=grid.extent, zorder=2,\n",
    "               cmap='cubehelix',\n",
    "               norm=colors.LogNorm(1, acc.max()),\n",
    "               interpolation='bilinear')\n",
    "plt.colorbar(im, ax=ax, label='Upstream Cells')\n",
    "plt.title('Flow Accumulation', size=14)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delineate basins and save the output to the \"output_basin_rasters\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from pysheds.grid import Grid\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "# Define the function to save the raster\n",
    "\n",
    "def save_raster(array, transform, output_path, crs):\n",
    "    with rasterio.open(\n",
    "        output_path, 'w', driver='GTiff',\n",
    "        height=array.shape[0], width=array.shape[1],\n",
    "        count=1, dtype=array.dtype, crs=crs, transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(array, 1)\n",
    "\n",
    "# Define the function to generate the basin raster\n",
    "\n",
    "def generate_basin_raster(pour_point, grid, fdir, output_raster_path):\n",
    "    x, y = pour_point.geometry.x.values[0], pour_point.geometry.y.values[0]\n",
    "    grid = Grid.from_raster(dem_raster, crs = 26910)\n",
    "    # Snap pour point to high accumulation cell\n",
    "    x_snap, y_snap = grid.snap_to_mask(acc > threshold, (x, y))\n",
    "\n",
    "    if x_snap is None or y_snap is None:\n",
    "        print(f\"No suitable cell found for pour point: ({x}, {y})\")\n",
    "        return\n",
    "\n",
    "    # Delineate the catchment\n",
    "    catch = grid.catchment(x=x_snap, y=y_snap, fdir=fdir, xytype='coordinate')\n",
    "\n",
    "    # Clip the grid to the catchment\n",
    "    grid.clip_to(catch)\n",
    "    catch_array = grid.view(catch)\n",
    "    transform = grid.affine\n",
    "    if catch_array.size == 0:\n",
    "        print(f\"No catchment area found for pour point: ({x_snap}, {y_snap})\")\n",
    "        return\n",
    "    \n",
    "    catch_array_numeric = catch_array.astype(np.uint8)\n",
    "\n",
    "    # Save raster\n",
    "    crs = \"EPSG:26910\"  # Adjust CRS as needed\n",
    "    save_raster(catch_array_numeric, transform, output_raster_path, crs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory setup\n",
    "output_dir = 'output_basin_rasters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the GeoDataFrame is in the correct CRS \n",
    "pour_points_clipped = pour_points_clipped.to_crs(epsg=26910)  \n",
    "\n",
    "\n",
    "# Iterate through each unique 'stationcod'\n",
    "for SITENO in pour_points_clipped[f'{pour_point_id}'].unique():\n",
    "    # Filter the pour points for the current SITENO\n",
    "    pour_point = pour_points_clipped[pour_points_clipped[f'{pour_point_id}'] == SITENO]\n",
    "    if pour_point.empty:\n",
    "        print(f\"No pour points found for SITENO: {SITENO}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate the output raster file path\n",
    "    output_raster_path = os.path.join(output_dir, f\"{SITENO}.tif\")\n",
    "    # Call the function to generate the basin raster\n",
    "    generate_basin_raster(pour_point, grid, fdir, output_raster_path)\n",
    "    \n",
    "    print(f\"Raster for SITENO {SITENO} saved to {output_raster_path}\")\n",
    "\n",
    "print(\"All rasters generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Spatial Join HRUs with Pour Point Catchment Areas</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "from rasterio.features import shapes\n",
    "\n",
    "# Path to the folder containing the raster files\n",
    "folder_path = 'output_basin_rasters'\n",
    "\n",
    "# List to store the geometries and file names\n",
    "geometries = []\n",
    "file_names = []\n",
    "\n",
    "# Loop through the folder and process each raster file\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.tif'):  # Adjust extension as necessary\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Read the raster data\n",
    "            image = src.read(1)  # Read the first band\n",
    "            \n",
    "            # Use rasterio's shapes function to create polygons from the raster\n",
    "            # We only want shapes where the value is 1\n",
    "            mask = image == 1\n",
    "            shapes_gen = shapes(image, mask=mask, transform=src.transform)\n",
    "            \n",
    "            # Create a list to store individual shapes\n",
    "            for geom, value in shapes_gen:\n",
    "                if value == 1:  # Ensure the shape is for value 1\n",
    "                    geom = shape(geom)  # Convert shape to a Shapely geometry\n",
    "                    geometries.append(geom)\n",
    "                    file_names.append(os.path.splitext(file_name)[0])\n",
    "        \n",
    "# Create a GeoDataFrame from the lists\n",
    "gdf = gpd.GeoDataFrame({\n",
    "    'filename': file_names,\n",
    "    'geometry': geometries\n",
    "}, crs=src.crs)  # Use the CRS of the raster file\n",
    "\n",
    "gdf = gdf.rename(columns={\"filename\": \"SITENO\"})\n",
    "\n",
    "# Print the GeoDataFrame to check the result\n",
    "print(gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output to geopackage\n",
    "\n",
    "gdf.to_file(\"generated_basins.gpkg\", layer = \"generated_basins\", driver='GPKG')\n",
    "pour_points_clipped.to_file(\"generated_basins.gpkg\", layer = \"pour_points\", driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Generated Basins and Pour Points\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the first GeoDataFrame\n",
    "gdf.plot(ax=ax, color='blue', alpha=0.5, edgecolor='k', label='Layer 1')\n",
    "\n",
    "# Plot the second GeoDataFrame\n",
    "pour_points_clipped.plot(ax=ax, color='red', alpha=0.5, edgecolor='k', label='Layer 2')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "# Set title and labels if needed\n",
    "ax.set_title('LOIs and Generated Basins')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Join model grid and generated basins for flow calculations\n",
    "master_hrus_by_LOIs = gpd.sjoin(model_grid_utmz10N,gdf, how='inner', predicate='intersects')\n",
    "master_hrus_by_LOIs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Create CSVs of each LOI with HRU IDs, Subbasin and StationCod</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder name\n",
    "folder_name = \"LOI_csvs\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Create the full path to the new folder\n",
    "input_csvs = os.path.join(current_directory, folder_name)\n",
    "\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(input_csvs):\n",
    "    # If not, create the folder\n",
    "    os.makedirs(input_csvs)\n",
    "    print(f\"Folder '{input_csvs}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{input_csvs}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the pour point ID in the beginning and save each group as a separate CSV file\n",
    "master_hrus_by_LOIs = master_hrus_by_LOIs.groupby(\"SITENO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the LOI and HRU IDs csv files\n",
    "master_hrus_by_LOIs.apply(lambda group: pd.DataFrame.to_csv(group, os.path.join(input_csvs, f'{group.name}.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code reads the csvs back, filters necessary columns and saves back to the same folder\n",
    "# Specify the file extension to filter for CSV files\n",
    "ext = ('.csv')\n",
    "\n",
    "# Loop through each file in the folder and process them\n",
    "for filename in os.listdir(input_csvs):\n",
    "    f = os.path.join(input_csvs, filename)\n",
    "    \n",
    "    # Check if the file has the specified extension\n",
    "    if f.endswith(ext):\n",
    "        # Split the file path to get the filename without extension\n",
    "        head_tail = os.path.split(f)\n",
    "        head_tail1 = input_csvs\n",
    "        k = head_tail[1]\n",
    "        r = k.split(\".\")[0]\n",
    "          \n",
    "        # Create a new file path for the processed CSV\n",
    "        p = os.path.join(head_tail1, f\"{r}.csv\")\n",
    "\n",
    "        # Read the CSV file\n",
    "        mydata = pd.read_csv(f)\n",
    "            \n",
    "        # Extract specific columns (pour_point_id, 'SUB_BASIN', 'HRU_ID') and save to a new CSV\n",
    "        new = mydata[[\"SITENO\", 'sub_basin', 'hru_id']]\n",
    "        new.to_csv(p, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Read Only Active HRU IDs from Model Grid Feature Class</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the field (column) name\n",
    "field_name = \"hru_id\"\n",
    "filter_columns = model_grid[field_name].tolist()\n",
    "\n",
    "# Now, field_values is a list containing all the values from the specified column\n",
    "print(filter_columns[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Read HRU Streamflow Out File</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not have access to snowflake, read the stream flow csv manually\n",
    "# hrus = pd.read_csv(\"EEL_HRU_STREAMFLOW_OUT_2002_2022.csv\") #change this to your streamflow csv\n",
    "# Fill in your snowflake connection parameters and retrieve the master hru streamflow table\n",
    "conn = snowflake.connector.connect( \n",
    "    account=\"\", # account name is found in the Single Sign On URL\n",
    "    user=\"\", #your waterboards email\n",
    "    authenticator=\"externalbrowser\",\n",
    "    role=\"DWR_DEV_RWC_SFROLE\", \n",
    "    warehouse=\"COMPUTE_WH\",  \n",
    "    database=\"DWR_DEV\",  \n",
    "    schema=\"INSTREAM_FLOWS_NC_DEV\")\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(hru_streamflow_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn streamflow table into a pandas dataframe for further manipulation.\n",
    "hrus = cur.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the last date is present and columns got read properly\n",
    "hrus_filtered = hrus.rename(columns={'HRU_ID': 'hru_id'})\n",
    "hrus_filtered.columns = hrus_filtered.columns.str.strip()\n",
    "hrus_filtered[\"hru_id\"] = hrus_filtered[\"hru_id\"].astype(int)\n",
    "hrus_filtered.set_index(\"hru_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<h2 style=\"background-color: #022851;\">\n",
    "<font size=\"+2\"><br>\n",
    "    <font color=#F8F8FF><b>&nbsp;   &nbsp; Output Modeled PRMS Unimpaired Flows by LOIs</b></font>\n",
    "    </font>  <br>\n",
    "</h2>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the input_csvs folder to see if it exists\n",
    "input_csvs = \"LOI_csvs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input csvs to a list so we can create a flows table for each one\n",
    "import glob\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(os.path.join(path, input_csvs, \"*.csv\"))\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder name\n",
    "output_folder = \"outFlows_by_LOIs\"\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Create the full path to the new folder\n",
    "output_flows_folder = os.path.join(current_directory, output_folder)\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(output_flows_folder):\n",
    "    # If not, create the folder\n",
    "    os.makedirs(output_flows_folder)\n",
    "    print(f\"Folder '{output_flows_folder}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{output_flows_folder}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming csv_files is a list of file paths and hrus_filtered is a DataFrame\n",
    "\n",
    "# Create a list of file names from the LOI_csvs folder\n",
    "fns = [os.path.splitext(os.path.basename(x))[0] for x in csv_files]\n",
    "\n",
    "# Read the csv files into dataframes\n",
    "d = {fn: pd.read_csv(csv_file) for fn, csv_file in zip(fns, csv_files)}\n",
    "\n",
    "# Extract HRU IDs from dataframes\n",
    "l = {key: value['hru_id'].astype(int).tolist() for key, value in d.items()}\n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "s = {}\n",
    "\n",
    "# Process HRU data to sum all rows in each column\n",
    "for key, value in l.items():\n",
    "    valid_ids = [vid for vid in value if vid in hrus_filtered.index]\n",
    "    if valid_ids:  # Only process if there are valid IDs\n",
    "        df_subset = hrus_filtered.loc[valid_ids]\n",
    "        # Compute the sum of all rows in each column\n",
    "        column_sums = df_subset.sum(axis=0)\n",
    "        # Store the result in the dictionary s\n",
    "        s[key] = pd.DataFrame(column_sums).T  # Transpose to get the sums in a single row\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(s.values(), ignore_index=True)\n",
    "combined_df.index = s.keys()  # Set index to be the keys from the original dictionary\n",
    "\n",
    "# Save the combined DataFrame to a single CSV file\n",
    "output_file = os.path.join(output_folder, \"combined_flows_cfs.csv\")\n",
    "combined_df.to_csv(output_file)\n",
    "print(f\"Saved combined data to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
